---
title: "ROC에 대한 짧은 글"
author: "Hwiwon Lee"
date: "`r format(Sys.Date())`"
categories:
  - Machine Learning
  - Book
tags:
  - R
  - Machine Learning
output: 
  # https://blog.zarathu.com/posts/2019-01-03-rmarkdown/
    bookdown::html_document2:
      number_sections: FALSE
      fig_caption: TRUE
      fig_height: 6
      fig_width: 10
      highlight: textmate
      theme: cosmo
      toc: yes
      toc_depth: 4
      toc_float: yes
      css: "post_style.css"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE, fig.height = 8, fig.align = "center", cache=T, dpi = 300, dev = "png",
                      comment = "#>")

# https://tibble.tidyverse.org/reference/formatting.html
options(tibble.print_max = 10)
options(tibble.max_extra_cols = 5)
# options(max.print = 300)

library(knitr) # for include_graphics()
library(tidyverse) # as you known, core package
library(googledrive) # Import dataset from the google drive
library(readr) # To read csv file
library(foreign)
library(caret)
library(showtext)
font_add_google('Noto Sans KR', 'notosanskr')
font_add_google('Noto Sans', 'notosans')
font_add_google('Nanum Gothic', 'nanumgothic')
```

# 여는 글
머신러닝을 하다보면 여러 모델의 성능을 비교하는 것이 일반적입니다. 오늘은 그 중에서도 이진 분류 문제(binary classification)에서 정말 많이 사용하는 ROC(Receiver Operating Characteristics) curve에 대해 얘기해보려 합니다.

ROC curve는 개념 상으로 크게 어려운 것이 없습니다만... 그 애매한 용어들과 비슷한 수식 때문에 한 번에 이해하기 쉽지 않습니다. 사실 ROC curve에 대한 완벽한 이해 없이 "ROC curve의 면적인 AUC(Area Under Curve)가 중요한 metric이고 이게 크면 좋다!"고 덮어놓고 가셔도 큰 문제는 없겠지만 누군가 "ROC curve가 뭐야?"라고 물었을 때, 데이터 분석가로서 있어보이는 한 마디는 해줘야 하지 않겠습니까? 

이 글의 목적이 바로 그렇습니다. "ROC curve가 뭐야?"에 대한 답변을 위한 짧은 글을 시작해보겠습니다. 

# ROC curve의 정의
Google에서 제공하는 machine learning crash course의 정의를 이용해 다음과 같이 ROC curve와 AUC를 정의할 수 있습니다. 

> ROC curve은 True Positive Rate(TPR), False Positive Rate(FPR)를 이용해 모든 분류 임계값에서 분류 모델의 성능을 보여주는 그래프이다. 이 때, 그려진 ROC curve 아래의 면적을 AUC(Area Under Curve)라 정의하며 분류 모델의 정확도를 보여주는 지표로 사용한다. 

위에서 보시다시피 ROC curve와 AUC는 분류 모델의 성능을 보여주는 지표 중 하나로 사용됩니다. 보통 **ROC curve를 그린다** 혹은 **ROC curve를 보자**는 말은 ROC curve와 AUC를 함께 검토하자는 의미로 사용되기도 합니다. 

ROC curve가 어떻게 분류 모델의 성능을 평가하는 지표로써 사용되는지 설명하기에 앞서 ROC curve를 이루는 중요한 개념들, **TPR**, **FPR**, **분류 임계값**에 대해 먼저 살펴보겠습니다. 




## True Positive Rate(TPR), False Positive Rate(FPR)
True Positive Rate(TPR), False Positive Rate(FPR)는 **confusion matrix**에서 구할 수 있는 값들로 분류 모델의 성능에 대한 지표로 사용할 수 있습니다. 

쉬운 설명을 위해 ROC curve가 실제로 많이 사용되는 이진 분류 모델을 사용한 질병 검진의 예시를 사용해서 설명해보겠습니다. 

|           |  질병 감염 여부 | |
|---|:---:|---:|

| 검진 결과 | 감염됨 | 감염되지 않음 | 합계 | 
|---|:---:|---:|---:|
| **양성** | True Positive(TP) | False Positive(FP) | TP + FP |
| **음성** | False Negative(FN) | True Negative(TN) | FN + TN | 
| 합계 | TP + FN |  FP + TN | TP + FP + FN + TN |

질병 검진 결과 위와 같은 confusion matrix를 얻었다고 했을 때, 기본적으로 사용할 수 있는 성능 지표에 대해 알아보겠습니다. 

- Accuracy : 전체 집단에서 실제로 감염된 대상을 양성으로, 비감염된 대상을 음성으로 제대로 판단한 비율
$$\frac{True\,Positive + True\,Negative}{Total}$$

- Error rate : 전체 집단에서 실제로 감염된 대상을 음성으로, 비 감염된 대상을 양성으로 잘못 판단한 비율
$$\frac{False\,Positive + False\,Negative}{Total} = 1 - Accuracy$$

Accuracy와 Error rate는 직관적으로 이해할 수 있는 개념들이며 옳게 보이기도 합니다. 그러나 **그룹간 불균형**이 심각한 상황, 가령, 희귀병에 대한 감염과 검진 결과로 만든 confusion matrix을 생각해봅시다. 

|           |  질병 감염 여부 | |
|---|:---:|---:|

| 검진 결과 | 감염됨 | 감염되지 않음 | 합계 | 
|---|:---:|---:|---:|
| **양성** | 4 | 3 | 7 |
| **음성** | 6 | 99,987 | 99,993 | 
| 합계 | 10 |  99,990 | 100,000 |

Accuracy는 `r round((99987+4)/100000, 2)`로 매우 높은 수준의 Accuracy를 갖지만 양성 집단의 분류 결과만 따로 보면 `r 4/10`으로 사용할 수 없는 수준입니다. 

이 모델이 제대로 작동하고 있다고 할 수 있을까요? Accuracy만 본다면 놀라운 수준의 모델이지만 이 포스트를 읽고 계신 여러분은 뭔가 잘 못 됐다는 느낌을 강하게 받고 계실겁니다. 이 문제를 해결하기 위해 고안해낸 것이 바로 True Positive Rate와 False Positive Rate입니다. 


- True Positive Rate(TPR, Sensitivity) : 실제로 감염인 집단을 대상으로 제대로(True) 양성(Positive)이라 판단한 비율
$$\frac{True\,Positive}{True\,Positive + False\,Negative}$$
- False Positive Rate(FPR) : 실제로 비 감염인 집단을 대상으로 잘못(False) 양성(Positive)으로 판단한 비율
$$\frac{False\,Positive}{False\,Positive + True\,Negative}$$
True Positive Rate, 그러니까 **TPR**는 쉽게 이해할 수 있습니다. 감염 상태인 집단을 대상으로 제대로 양성 진단을 내린 비율이므로 앞서 본 **Accuracy**와 비슷한 측면도 있습니다. 

문제는 False Positive Rate, **FPR**입니다. FPR의 개념을 이해하기 어려운 것은 아닙니다. 잘못 양성으로 진단한 비율이라면 FPR이 낮을수록 정확도가 높다는 말일태니까요. 

근데 TPR과 FRP, 이 두 값으로는 모델의 성능을 판단하기엔 뭔가 부족한 느낌이 듭니다. 이 개념을 잘 가지고 **임계값(threshold)**에 대해 알아봅시다. 

## 임계값과 TPR, FRP

