---
title: "ROC에 대한 짧은 글"
author: "Hwiwon Lee"
date: "`r format(Sys.Date())`"
categories:
  - Machine Learning
  - Book
tags:
  - R
  - Machine Learning
output: 
  # https://blog.zarathu.com/posts/2019-01-03-rmarkdown/
    bookdown::html_document2:
      number_sections: FALSE
      fig_caption: TRUE
      fig_height: 6
      fig_width: 10
      highlight: textmate
      theme: cosmo
      toc: yes
      toc_depth: 4
      toc_float: yes
      css: "post_style.css"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE, fig.height = 8, fig.align = "center", cache=T, dpi = 300, dev = "png",
                      comment = "#>")

# https://tibble.tidyverse.org/reference/formatting.html
options(tibble.print_max = 10)
options(tibble.max_extra_cols = 5)
# options(max.print = 300)

library(knitr) # for include_graphics()
library(tidyverse) # as you known, core package
library(googledrive) # Import dataset from the google drive
library(readr) # To read csv file
library(foreign)
library(caret)
library(showtext)
font_add_google('Noto Sans KR', 'notosanskr')
font_add_google('Noto Sans', 'notosans')
font_add_google('Nanum Gothic', 'nanumgothic')
font_add_google('Lora', 'test')
```

# 여는 글
머신러닝을 하다보면 여러 모델의 성능을 비교하는 것이 일반적입니다. 오늘은 그 중에서도 이진 분류 문제(binary classification)에서 정말 많이 사용하는 ROC(Receiver Operating Characteristics) curve에 대해 얘기해보려 합니다.

ROC curve는 개념 상으로 크게 어려운 것이 없습니다만... 그 애매한 용어들과 비슷한 수식 때문에 한 번에 이해하기 쉽지 않습니다. 사실 ROC curve에 대한 완벽한 이해 없이 "ROC curve의 면적인 AUC(Area Under Curve)가 중요한 metric이고 이게 크면 좋다!"고 덮어놓고 가셔도 큰 문제는 없겠지만 누군가 "ROC curve가 뭐야?"라고 물었을 때, 데이터 분석가로서 있어보이는 한 마디는 해줘야 하지 않겠습니까? 

이 글의 목적이 바로 그렇습니다. "ROC curve가 뭐야?"에 대한 답변을 위한 짧은 글을 시작해보겠습니다. 

# ROC curve의 정의
Google에서 제공하는 machine learning crash course의 정의를 이용해 다음과 같이 ROC curve와 AUC를 정의할 수 있습니다. 

> ROC curve은 True Positive Rate(TPR), False Positive Rate(FPR)를 이용해 모든 분류 임계값에서 분류 모델의 성능을 보여주는 그래프이다. 이 때, 그려진 ROC curve 아래의 면적을 AUC(Area Under Curve)라 정의하며 분류 모델의 정확도를 보여주는 지표로 사용한다. 

위에서 보시다시피 ROC curve와 AUC는 분류 모델의 성능을 보여주는 지표 중 하나로 사용됩니다. 보통 **ROC curve를 그린다** 혹은 **ROC curve를 보자**는 말은 ROC curve와 AUC를 함께 검토하자는 의미로 사용되기도 합니다. 

ROC curve가 어떻게 분류 모델의 성능을 평가하는 지표로써 사용되는지 설명하기에 앞서 ROC curve를 이루는 중요한 개념들, **TPR**, **FPR**, **분류 임계값**에 대해 먼저 살펴보겠습니다. 




## True Positive Rate(TPR), False Positive Rate(FPR)
True Positive Rate(TPR), False Positive Rate(FPR)는 **confusion matrix**에서 구할 수 있는 값들로 분류 모델의 성능에 대한 지표로 사용할 수 있습니다. 

쉬운 설명을 위해 ROC curve가 실제로 많이 사용되는 이진 분류 모델을 사용한 질병 검진의 예시를 사용해서 설명해보겠습니다. 

|           |  질병 감염 여부 | |
|---|:---:|---:|

| 검진 결과 | 감염됨 | 감염되지 않음 | 합계 | 
|---|:---:|---:|---:|
| **양성** | True Positive(TP) | False Positive(FP) | TP + FP |
| **음성** | False Negative(FN) | True Negative(TN) | FN + TN | 
| 합계 | TP + FN |  FP + TN | TP + FP + FN + TN |

질병 검진 결과 위와 같은 confusion matrix를 얻었다고 했을 때, 기본적으로 사용할 수 있는 성능 지표에 대해 알아보겠습니다. 

- Accuracy : 전체 집단에서 실제로 감염된 대상을 양성으로, 비감염된 대상을 음성으로 제대로 판단한 비율
$$\frac{True\,Positive + True\,Negative}{Total}$$

- Error rate : 전체 집단에서 실제로 감염된 대상을 음성으로, 비 감염된 대상을 양성으로 잘못 판단한 비율
$$\frac{False\,Positive + False\,Negative}{Total} = 1 - Accuracy$$

Accuracy와 Error rate는 직관적으로 이해할 수 있는 개념들이며 옳게 보이기도 합니다. 그러나 **그룹간 불균형**이 심각한 상황, 가령, 희귀병에 대한 감염과 검진 결과로 만든 confusion matrix을 생각해봅시다. 

|           |  질병 감염 여부 | |
|---|:---:|---:|

| 검진 결과 | 감염됨 | 감염되지 않음 | 합계 | 
|---|:---:|---:|---:|
| **양성** | 4 | 3 | 7 |
| **음성** | 6 | 99,987 | 99,993 | 
| 합계 | 10 |  99,990 | 100,000 |

Accuracy는 `r round((99987+4)/100000, 2)`로 매우 높은 수준의 Accuracy를 갖지만 양성 집단의 분류 결과만 따로 보면 `r 4/10`으로 사용할 수 없는 수준입니다. 

이 모델이 제대로 작동하고 있다고 할 수 있을까요? Accuracy만 본다면 놀라운 수준의 모델이지만 이 포스트를 읽고 계신 여러분은 뭔가 잘 못 됐다는 느낌을 강하게 받고 계실겁니다. 이 문제를 해결하기 위해 고안해낸 것이 바로 True Positive Rate와 False Positive Rate입니다. 


- True Positive Rate(TPR, Sensitivity) : 실제로 감염인 집단을 대상으로 제대로(True) 양성(Positive)이라 판단한 비율
$$\frac{True\,Positive}{True\,Positive + False\,Negative}$$
- False Positive Rate(FPR) : 실제로 비 감염인 집단을 대상으로 잘못(False) 양성(Positive)으로 판단한 비율
$$\frac{False\,Positive}{False\,Positive + True\,Negative}$$
True Positive Rate, 그러니까 **TPR**는 쉽게 이해할 수 있습니다. 감염 상태인 집단을 대상으로 제대로 양성 진단을 내린 비율이므로 앞서 본 **Accuracy**와 비슷한 측면도 있습니다. 

문제는 False Positive Rate, **FPR**입니다. FPR의 개념을 이해하기 어려운 것은 아닙니다. 잘못 양성으로 진단한 비율이라면 FPR이 낮을수록 정확도가 높다는 말일태니까요. 

근데 TPR과 FRP, 이 두 값으로는 모델의 성능을 판단하기엔 뭔가 부족한 느낌이 듭니다. 이 개념을 잘 가지고 **임계값(threshold)**에 대해 알아봅시다. 

## 임계값과 TPR, FRP
임계값과 TPR, FRP를 쉽게 설명하기 위해 예제와 여러 플롯을 사용해보겠습니다. 먼저 다음과 같은 상황을 가정해봅시다. 

- 임의의 질병 "X"의 감염 여부를 판단하는 모델을 만들었다. 

- 질병 감염 여부를 판단하기 위해 2천 명의 대상자들을 모집했다. 

- 이 가운데, 1천 명은 "X에 감염된 상태이고 1천 명은 "X"에 감염되지 않은 상태이다. 

위의 상황에서 이상적인 모델은 감염 여부를 정확히 밝혀내 감염자 1천 명, 비 감염자 1천 명으로 구분해내는 모델일 것입니다.

```{r, fig.showtext=TRUE, fig.cap="모델이 감염자와 비 감염자를 완벽히 구분할 수 있는 경우의 density plot. 임계값(threshold)를 0.5로 설정한다면 감염자와 비 감염자를 정확하게 구분할 수 있다."}

showtext_auto()
windows()

set.seed(1234)
normal <- rnorm(1000, mean = 0.3, sd = 0.05)
disease <- rnorm(1000, mean = 0.7, sd = 0.05)

dt <- as.data.frame(cbind(normal, disease, index = seq(1, 1000, 1)))

dt %>% 
  pivot_longer(cols = -index, 
               names_to = "Status") %>% 
  mutate(Status = factor(Status, levels = c("normal", "disease"), labels = c("Normal", "Disease"))) %>% 
  ggplot(aes(x = value, 
             fill = Status, colour = Status)) +
  # density가 갑자기 헷갈린다. 
  # densisy면 0부터 1이어야 하는 거 아닌가? kernal density라 아님. 
  # https://stackoverflow.com/questions/51385455/geom-density-y-axis-goes-above-1
  # https://stats.stackexchange.com/questions/172311/r-geom-density-values-in-y-axis
  # https://stackoverflow.com/questions/15664202/how-to-interpret-the-different-ggplot2-densities
  geom_density(alpha = 0.1) + 
  scale_x_continuous(name = "Threshold", 
                     limits = c(0, 1),
                     breaks = c(0, seq(0.1, 0.9, 0.2), 1), 
                     labels = as.character(c(0, seq(0.1, 0.9, 0.2), 1)), 
                     expand = c(0.01, 0.01)
  ) + 
  scale_y_continuous(name = "Density", 
                     expand = c(0.01, 0.01)
  ) + 
  geom_vline(xintercept = 0.5, 
             color = "red", 
             size = 1.5, 
             alpha = 0.5) + 
  annotate("text", x=0.3, y=6, label="True Negative", size=8, family = "notosans") + 
  annotate("text", x=0.7, y=6, label="True Positive", size=8, family = "notosans") + 
  theme_minimal() + 
  theme(text = element_text(family = "notosans"))
```



```{r}
# https://datascienceschool.net/view-notebook/d0df94cf8dd74e8daec7983531f68dfc/
```



