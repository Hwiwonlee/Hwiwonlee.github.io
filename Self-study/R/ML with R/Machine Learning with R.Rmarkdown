---
title: "Machine Learning with R"
author: "Hwiwon Lee"
date: "`r format(Sys.Date())`"
categories:
  - Machine Learning
  - Book
tags:
  - R
  - Machine Learning
output: 
  # https://blog.zarathu.com/posts/2019-01-03-rmarkdown/
    bookdown::html_document2:
      number_sections: FALSE
      fig_caption: TRUE
      fig_height: 6
      fig_width: 10
      highlight: textmate
      theme: cosmo
      toc: yes
      toc_depth: 4
      toc_float: yes
      css: "post_style.css"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE, fig.height = 8, fig.align = "center", cache=T, dpi = 300, dev = "png",
                      comment = "#>")

# https://tibble.tidyverse.org/reference/formatting.html
options(tibble.print_max = 10)
options(tibble.max_extra_cols = 5)
# options(max.print = 300)

library(knitr) # for include_graphics()
library(tidyverse) # as you known, core package
library(googledrive) # Import dataset from the google drive
library(readr) # To read csv file
library(foreign)
library(caret)
library(showtext)
font_add_google('Noto Sans KR', 'notosanskr')
font_add_google('Noto Sans', 'notosans')
font_add_google('Nanum Gothic', 'nanumgothic')

```

# 3장 게으른 학습 : 최근접 이웃을 사용한 분류(Lazy Learning : Classification with KNN Method)

3장에서는 [위스콘신 유방암 진단 데이터셋](http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/)을 대상으로 [K-Nearest Neighbors algorithm](https://ko.wikipedia.org/wiki/K-%EC%B5%9C%EA%B7%BC%EC%A0%91_%EC%9D%B4%EC%9B%83_%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98)(KNN, K-최근접 이웃 방법)를 이용한 분류에 대해 다루고 있다.  

머신러닝을 시작할 때, KNN으로 시작하는 경우가 많은데 아무래도 분포나 통계적인 개념들보다 **거리**같은 다소 익숙한 수치(metric)으로 접근 가능하기 때문이 아닌가 생각한다. 그러나 위의 위키피디아 링크를 보면 사실 KNN도 그렇게 쉬운 알고리즘은 아니다. 

이 포스트는 KNN에 대한 이론적인 접근보다는 R에서 KNN을 어떻게 사용하는지에 집중하는 책의 내용처럼 **R에서 KNN을 어떻게 사용할 것인가**를 주로 다룰 것이다. 더하여, 책에서 다루지 않은 **caret 패키지**를 이용한 KNN과 결과의 시각화를 연습해보는 것에 그 목적을 두고 있다. 

## 3.1 WDBC 데이터셋 
[위스콘신 유방암 진단 데이터셋](http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/)에서 받을 수 있는 데이터셋은 `breast-cancer-wisconsin`, `wdbc`(Wisconsin Diagnostic Breast Cancer), `wpbc`(Wisconsin Prognostic Breast Cancer) 등, 총 3개며 각각의 index에 데이터셋의 간단한 설명이 있다. 

R을 활용한 머신러닝에서 사용된 데이터셋은 `wdbc`로 종양에서 진단 혹은 측정할 수 있는 대한 10개의 항목에 대한 평균, 표준 오차, 최댓값을 포함하고 있으며 진단 결과 양성인지 악성인지에 대한 정보 또한 갖고 있다. 따라서 `wdbc`는 `id`를 포함한 32개의 feature를 갖는다. 

사소한 문제 중 하나는 위의 [위스콘신 유방암 진단 데이터셋](http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/)에서 받을 수 있는 `wdbc`에는 feature 이름이 빠져 있다는 것이다. 다행히도 [구글링](https://resources.oreilly.com/examples/9781784393908/blob/ac9fe41596dd42fc3877cfa8ed410dd346c43548/Machine%20Learning%20with%20R,%20Second%20Edition_Code/Chapter%2003/wisc_bc_data.csv)(R을 활용한 머신러닝 2판의 저장소)을 통해 알맞는 이름이 붙여진 데이터셋을 쉽게 구할 수 있다. 

```{r, echo=FALSE, include=FALSE}
drive_auth()
data_in_drive <- drive_find(type = "csv", pattern = "wdbc")

for(i in 1:1){ 
  drive_download(file = data_in_drive$name[i], path = data_in_drive$name[i], overwrite = TRUE)
}
```

```{r}
wdbc <- read_csv("wdbc.csv", col_names = TRUE)
wdbc
```
569x32를 가진 `wdbc`를 성공적으로 불러왔다. **거리**를 사용하는 KNN 알고리즘은 feature들의 표준화가 매우 중요하므로 `preProcess()`를 이용해 centering과 scaling을 해보자. 

## 3.2 머신러닝을 위한 전처리
이번 KNN 예제에서는 train, test set으로 나누는 것과 centering, scaling 변환을 이용해 데이터셋 전처리를 해보겠다.
```{r data partition to train and test set using caret pacakge}
set.seed(1234)

# createDataPartition()를 이용해 75:25로 train, test 나누기 
indextrain <- createDataPartition(y = wdbc$diagnosis,
                                  p = 0.75,
                                  list = FALSE)
# train, test 선언 
# id column은 제외한다. 
train <- wdbc[indextrain, -1]
test <- wdbc[-indextrain, -1]
```

`createDataPartition()`로 dataset split을 진행하면 train, test에 포함될 종속변수의 비율을 원래의 데이터셋과 최대한 같게 유지시켜주기 때문에 train, test set 사이에 발생할 수도 있는 종속변수의 불균형에 대해 신경쓰지 않아도 된다는 장점이 있다. 

```{r, collapse=TRUE}
# train set의 diagnosis 비율
train %>% 
  count(diagnosis) %>% 
  mutate(n = n/nrow(train))

# test set의 diagnosis 비율
test %>% 
  count(diagnosis) %>% 
  mutate(n = n/nrow(test))

# wdbc의 diagnosis 비율
wdbc %>% 
  count(diagnosis) %>% 
  mutate(n = n/nrow(wdbc))
```
train, test로 데이터셋을 나누었다면 trasformation을 해보자. 책에서 소개되는 transformation 방법은 min-max normalization과 z-score를 이용한 standardization, 두 가지인데 두 방법 모두 해보고 더 나은 것을 사용하거나 경우에 따라 또 다른 전처리 방법을 이용하기도 한다. 

먼저 z-score를 이용한 standardization로 transformation 해보고 후에 min-max normalization을 해보도록 하자. 

```{r centering and scaling with preProcess()}
# preProcess()를 이용한 transformation
# center : feature의 평균이 0이 되게 변환
# scale : feature의 표준편차가 1이 되게 변환
prep <- preProcess(train, method = c("center", "scale"))

train_st <- predict(prep, train)

# train의 평균과 표준편차로 standardization
test_st <- predict(prep, test)
```


이 쯤에서 머신러닝에 대한 이해를 위해 split과 transformation에 대한 중요한 개념을 설명해야 할 것 같다. 

### 3.2.1 dataset split 이후 데이터 변환(trasformation)을 하는 이유

train, test으로 나눠(여기에 validation set까지 나누는 경우도 포함해서) 분석하는 머신러닝이 익숙하지 않은 데이터 분석가들이 가질 수 있는 궁금증 중 하나가 바로 "왜 dataset split 이후에 데이터 변환을 하는가?" 이다. 

의문을 갖는 이유가 original dataset에서 먼저 standardization하고 split을 하면 상대적으로 짧고 효율적으로 처리할 수 있다는 것일 수도 있고 그냥 막연히 궁금하기 때문일 수도 있다. 이유가 무엇이든, 이 질문은 머신러닝의 개념을 이해하는 데 상당히 중요하다. 

통상적인 머신러닝은 원래 자료를 train, test으로 나눠 train set을 이용해 적절한 model을 구하고 이를 test에 fitting시켜 얼만큼의 performance를 보이는지 알아보는 것으로 진행됨을 알고 있을 것이다. train, test으로 데이터셋을 나누는 이유는 데이터 안에 모델이 갖혀버리는 것을 방지해 궁극적으로는 같은 문제를 가진 완전히 다른 데이터에서도 높은 수준의 performance를 보이는 model을 만들기 위함이다. 

위의 목적에 기반해 model의 실제 성능을 평가하는 **test set**의 역할을 매우 중요하게 생각하는 것이 일반적이며 이를 위해 이론적으로 **test set**은 **한 번도 공개되지 않은 미지의 데이터 셋**이어야 한다. 

물론, 우리가 가진 orginal dataset에서 얼마를 툭 잘라 test set이라 놓았지만 그럼에도 불구하고 test set은 반드시 **미지의 데이터셋** 상태여야 한다. 이 상태는 test을 사용하기 이전까지는 **test의 정보가 어디에서도 사용되서는 안된다**는 말로 바꿔 이해할 수 있다. 

전처리의 과정은 transformation에 이 개념을 적용해보자. 전체 데이터셋을 대상으로 transformation을 진행하면 **test set이 될** 관측치를 사용하게 되며 이는 미지의 데이터셋이라는 test set의 조건을 만족시킬 수 없게 된다. 

따라서 data split을 먼저 진행한 후, train set에 대한 transformation을 시행한다. 그 후, 우리가 알지 못하는 test에 대한 transformation을 수행하는데 transformation 방법으로 standardization을 가정하면 **test의 transformation에 사용할 값**은 **train set의 평균과 표준 편차**다. 이유는 앞서 말한대로 test set이 미지의 상태라 가정되었기 때문이고 더불어 test의 평균과 표준 편차를 알 수 없다는 극단적인 상황을 가정하기 위함이기도 하다. 

실제로, train set의 평균과 표준편차로 test의 standardization을 수행하면 평균 0, 표준 편차 1이 나오지 않는다. 

```{r, collapse=TRUE}
train_st[, -1] %>% 
  select(sort(names(.))) %>% 
  mutate(id = as.character(seq(1, nrow(.), 1))) %>% 
  select(id, everything()) %>% 
  pivot_longer(-id) %>% 
  mutate(name = str_remove(name, "_.*")) %>% 
  mutate(metric = rep(c("mean", "se", "max"), nrow(.)/3)) %>% 
  group_by(name, metric) %>% 
  dplyr::summarise(across(where(is.numeric), list(mean = ~round(mean(.x), 4), sd = ~round(sd(.x), 4))))
  

test_st[, -1] %>% 
  select(sort(names(.))) %>% 
  mutate(id = as.character(seq(1, nrow(.), 1))) %>% 
  select(id, everything()) %>% 
  pivot_longer(-id) %>% 
  mutate(name = str_remove(name, "_.*")) %>% 
  mutate(metric = rep(c("mean", "se", "max"), nrow(.)/3)) %>% 
  group_by(name, metric) %>% 
  dplyr::summarise(across(where(is.numeric), list(mean = ~round(mean(.x), 4), sd = ~round(sd(.x), 4))))
```
아주 좋은 상황인 train set에서 구현된 model이 최적의 조건이 아닌 test set에서도 높은 수준의 성능을 보인다면 해당 model은 훌륭한 model이라 평가할 수 있을 것이다. 이러한 상황을 구현해내기 위해 data split 이후 transformation을 진행하는 것이다. 

- 참고
    - [Difference between preprocessing train and test set before and after splitting](https://stats.stackexchange.com/questions/267012/difference-between-preprocessing-train-and-test-set-before-and-after-splitting)
    - [Stndardization, Normalization, Minmax(표준화, 정규화, 민맥스)](https://hwiyong.tistory.com/91)
    - [Feature Scaling and Normalisation in a nutshell](https://medium.com/analytics-vidhya/feature-scaling-and-normalisation-in-a-nutshell-5319af86f89b)
    

