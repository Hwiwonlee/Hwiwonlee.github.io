---
title: "Machine Learning with R"
author: "Hwiwon Lee"
date: "`r format(Sys.Date())`"
categories:
  - Machine Learning
  - Book
tags:
  - R
  - Machine Learning
output: 
  # https://blog.zarathu.com/posts/2019-01-03-rmarkdown/
    bookdown::html_document2:
      number_sections: FALSE
      fig_caption: TRUE
      fig_height: 6
      fig_width: 10
      highlight: textmate
      theme: cosmo
      toc: yes
      toc_depth: 4
      toc_float: yes
      css: "post_style.css"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE, fig.height = 8, fig.align = "center", cache=T, dpi = 300, dev = "png",
                      comment = "#>")

# https://tibble.tidyverse.org/reference/formatting.html
options(tibble.print_max = 10)
options(tibble.max_extra_cols = 5)
# options(max.print = 300)

library(knitr) # for include_graphics()
library(tidyverse) # as you known, core package
library(googledrive) # Import dataset from the google drive
library(readr) # To read csv file
library(foreign)
library(caret)
library(VennDiagram)
library(venneuler)
library(showtext)
font_add_google('Noto Sans KR', 'notosanskr')
font_add_google('Noto Sans', 'notosans')
font_add_google('Nanum Gothic', 'nanumgothic')
```

# 3장 게으른 학습 : 최근접 이웃을 사용한 분류(Lazy Learning : Classification with KNN Method)

3장에서는 [위스콘신 유방암 진단 데이터셋](http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/)을 대상으로 [K-Nearest Neighbors algorithm](https://ko.wikipedia.org/wiki/K-%EC%B5%9C%EA%B7%BC%EC%A0%91_%EC%9D%B4%EC%9B%83_%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98)(KNN, K-최근접 이웃 방법)를 이용한 분류에 대해 다루고 있다.  

머신러닝을 시작할 때, KNN으로 시작하는 경우가 많은데 아무래도 분포나 통계적인 개념들보다 **거리**같은 다소 익숙한 수치(metric)으로 접근 가능하기 때문이 아닌가 생각한다. 그러나 위의 위키피디아 링크를 보면 사실 KNN도 그렇게 쉬운 알고리즘은 아니다. 

이 포스트는 KNN에 대한 이론적인 접근보다는 R에서 KNN을 어떻게 사용하는지에 집중하는 책의 내용처럼 **R에서 KNN을 어떻게 사용할 것인가**를 주로 다룰 것이다. 더하여, 책에서 다루지 않은 **caret 패키지**를 이용한 KNN과 결과의 시각화를 연습해보는 것에 그 목적을 두고 있다. 

## 3.1 WDBC 데이터셋 
[위스콘신 유방암 진단 데이터셋](http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/)에서 받을 수 있는 데이터셋은 `breast-cancer-wisconsin`, `wdbc`(Wisconsin Diagnostic Breast Cancer), `wpbc`(Wisconsin Prognostic Breast Cancer) 등, 총 3개며 각각의 index에 데이터셋의 간단한 설명이 있다. 

R을 활용한 머신러닝에서 사용된 데이터셋은 `wdbc`로 종양에서 진단 혹은 측정할 수 있는 대한 10개의 항목에 대한 평균, 표준 오차, 최댓값을 포함하고 있으며 진단 결과 양성인지 악성인지에 대한 정보 또한 갖고 있다. 따라서 `wdbc`는 `id`를 포함한 32개의 feature를 갖는다. 

사소한 문제 중 하나는 위의 [위스콘신 유방암 진단 데이터셋](http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/)에서 받을 수 있는 `wdbc`에는 feature 이름이 빠져 있다는 것이다. 다행히도 [구글링](https://resources.oreilly.com/examples/9781784393908/blob/ac9fe41596dd42fc3877cfa8ed410dd346c43548/Machine%20Learning%20with%20R,%20Second%20Edition_Code/Chapter%2003/wisc_bc_data.csv)(R을 활용한 머신러닝 2판의 저장소)을 통해 알맞는 이름이 붙여진 데이터셋을 쉽게 구할 수 있다. 

```{r, echo=FALSE, include=FALSE}
drive_auth()
data_in_drive <- drive_find(type = "csv", pattern = "wdbc")

for(i in 1:1){ 
  drive_download(file = data_in_drive$name[i], path = data_in_drive$name[i], overwrite = TRUE)
}
```

```{r}
wdbc <- read_csv("wdbc.csv", col_names = TRUE)
wdbc
```
569x32를 가진 `wdbc`를 성공적으로 불러왔다. **거리**를 사용하는 KNN 알고리즘은 feature들의 표준화가 매우 중요하므로 `preProcess()`를 이용해 centering과 scaling을 해보자. 

## 3.2 머신러닝을 위한 전처리
이번 KNN 예제에서는 train, test set으로 나누는 것과 centering, scaling 변환을 이용해 데이터셋 전처리를 해보겠다.
```{r data partition to train and test set using caret pacakge}
set.seed(1234)

# createDataPartition()를 이용해 75:25로 train, test 나누기 
indextrain <- createDataPartition(y = wdbc$diagnosis,
                                  p = 0.75,
                                  list = FALSE)
# train, test 선언 
# id column은 제외한다. 
train <- wdbc[indextrain, -1]
test <- wdbc[-indextrain, -1]
```

`createDataPartition()`로 dataset split을 진행하면 train, test에 포함될 종속변수의 비율을 원래의 데이터셋과 최대한 같게 유지시켜주기 때문에 train, test set 사이에 발생할 수도 있는 종속변수의 불균형에 대해 신경쓰지 않아도 된다는 장점이 있다. 

```{r, collapse=TRUE}
# train set의 diagnosis 비율
train %>% 
  count(diagnosis) %>% 
  mutate(n = n/nrow(train))

# test set의 diagnosis 비율
test %>% 
  count(diagnosis) %>% 
  mutate(n = n/nrow(test))

# wdbc의 diagnosis 비율
wdbc %>% 
  count(diagnosis) %>% 
  mutate(n = n/nrow(wdbc))
```
train, test로 데이터셋을 나누었다면 trasformation을 해보자. 책에서 소개되는 transformation 방법은 min-max normalization과 z-score를 이용한 standardization, 두 가지인데 두 방법 모두 해보고 더 나은 것을 사용하거나 경우에 따라 또 다른 전처리 방법을 이용하기도 한다. 

이번에는 흔히 쓰이는 먼저 z-score를 이용한 standardization로 transformation를 사용해보겠다.

```{r centering and scaling with preProcess()}
# preProcess()를 이용한 transformation
# center : feature의 평균이 0이 되게 변환
# scale : feature의 표준편차가 1이 되게 변환
prep <- preProcess(train, method = c("center", "scale"))

train_st <- predict(prep, train)

# train의 평균과 표준편차로 standardization
test_st <- predict(prep, test)
```


이 쯤에서 머신러닝에 대한 이해를 위해 split과 transformation에 대한 중요한 개념을 설명해야 할 것 같다. 

### 3.2.1 dataset split 이후 데이터 변환(trasformation)을 하는 이유

train, test으로 나눠(여기에 validation set까지 나누는 경우도 포함해서) 분석하는 머신러닝이 익숙하지 않은 데이터 분석가들이 가질 수 있는 궁금증 중 하나가 바로 "왜 dataset split 이후에 데이터 변환을 하는가?" 이다. 

의문을 갖는 이유가 original dataset에서 먼저 standardization하고 split을 하면 상대적으로 짧고 효율적으로 처리할 수 있다는 것일 수도 있고 그냥 막연히 궁금하기 때문일 수도 있다. 이유가 무엇이든, 이 질문은 머신러닝의 개념을 이해하는 데 상당히 중요하다. 

통상적인 머신러닝은 원래 자료를 train, test으로 나눠 train set을 이용해 적절한 model을 구하고 이를 test에 fitting시켜 얼만큼의 performance를 보이는지 알아보는 것으로 진행됨을 알고 있을 것이다. train, test으로 데이터셋을 나누는 이유는 데이터 안에 모델이 갖혀버리는 것을 방지해 궁극적으로는 같은 문제를 가진 완전히 다른 데이터에서도 높은 수준의 performance를 보이는 model을 만들기 위함이다. 

위의 목적에 기반해 model의 실제 성능을 평가하는 **test set**의 역할을 매우 중요하게 생각하는 것이 일반적이며 이를 위해 이론적으로 **test set**은 **한 번도 공개되지 않은 미지의 데이터 셋**이어야 한다. 

물론, 우리가 가진 orginal dataset에서 얼마를 툭 잘라 test set이라 놓았지만 그럼에도 불구하고 test set은 반드시 **미지의 데이터셋** 상태여야 한다. 이 상태는 test을 사용하기 이전까지는 **test의 정보가 어디에서도 사용되서는 안된다**는 말로 바꿔 이해할 수 있다. 

전처리의 과정은 transformation에 이 개념을 적용해보자. 전체 데이터셋을 대상으로 transformation을 진행하면 **test set이 될** 관측치를 사용하게 되며 이는 미지의 데이터셋이라는 test set의 조건을 만족시킬 수 없게 된다. 

따라서 data split을 먼저 진행한 후, train set에 대한 transformation을 시행한다. 그 후, 우리가 알지 못하는 test에 대한 transformation을 수행하는데 transformation 방법으로 standardization을 가정하면 **test의 transformation에 사용할 값**은 **train set의 평균과 표준 편차**다. 이유는 앞서 말한대로 test set이 미지의 상태라 가정되었기 때문이고 더불어 test의 평균과 표준 편차를 알 수 없다는 극단적인 상황을 가정하기 위함이기도 하다. 

실제로, train set의 평균과 표준편차로 test의 standardization을 수행하면 평균 0, 표준 편차 1이 나오지 않는다. 

```{r, collapse=TRUE}
train_st[, -1] %>% 
  select(sort(names(.))) %>% 
  mutate(id = as.character(seq(1, nrow(.), 1))) %>% 
  select(id, everything()) %>% 
  pivot_longer(-id) %>% 
  mutate(name = str_remove(name, "_.*")) %>% 
  mutate(metric = rep(c("mean", "se", "max"), nrow(.)/3)) %>% 
  group_by(name, metric) %>% 
  dplyr::summarise(across(where(is.numeric), list(mean = ~round(mean(.x), 4), sd = ~round(sd(.x), 4))))
  

test_st[, -1] %>% 
  select(sort(names(.))) %>% 
  mutate(id = as.character(seq(1, nrow(.), 1))) %>% 
  select(id, everything()) %>% 
  pivot_longer(-id) %>% 
  mutate(name = str_remove(name, "_.*")) %>% 
  mutate(metric = rep(c("mean", "se", "max"), nrow(.)/3)) %>% 
  group_by(name, metric) %>% 
  dplyr::summarise(across(where(is.numeric), list(mean = ~round(mean(.x), 4), sd = ~round(sd(.x), 4))))
```
아주 좋은 상황인 train set에서 구현된 model이 최적의 조건이 아닌 test set에서도 높은 수준의 성능을 보인다면 해당 model은 훌륭한 model이라 평가할 수 있을 것이다. 이러한 상황을 구현해내기 위해 data split 이후 transformation을 진행하는 것이다. 

- 참고
    - [Difference between preprocessing train and test set before and after splitting](https://stats.stackexchange.com/questions/267012/difference-between-preprocessing-train-and-test-set-before-and-after-splitting)
    - [Stndardization, Normalization, Minmax(표준화, 정규화, 민맥스)](https://hwiyong.tistory.com/91)
    - [Feature Scaling and Normalisation in a nutshell](https://medium.com/analytics-vidhya/feature-scaling-and-normalisation-in-a-nutshell-5319af86f89b)
    


## 3.3 training
책에서는 `knn()`을 이용해 여러 `k` 값을 직접 설정하며 training을 했지만 **caret** 패키지의 `trainControl()`과 `train()`을 이용하면 꽤 간편하게 이 과정을 진행할 수 있다. 

먼저 training을 어떤 방법으로 진행할 것인지에 대한 `trainControl()`를 먼저 정의하자. `trainControl()`는 대부분의 training method를 지원하고 있으며 자세한 내용은 [`trainControl()` 페이지](https://rdrr.io/rforge/caret/man/trainControl.html)를 참고하자. 

이번에는 K-Fold Cross-validation을 일정 횟수 만큼 반복하는 Repeated CV를 이용할 것이며 fold는 10개, 반복 수에는 5번을 설정했다. 

```{r trainControl, collapse=TRUE}

# repeated cv를 진행
# fold는 10개, 반복 수는 5번으로 설정
traincontrol <- trainControl(method = "repeatedcv", 
                             number = 10, 
                             repeats = 5)
```

만든 `traincontrol`과 `train()`을 이용해 training을 해보자. `train()`은 caret 패키지에서 지원하는 training 함수로 정말 많은 수의 모델에 대한 학습을 지원하고 있으며 각 모델에 따라 사용하는 parameter 또한 달라지므로 사용할 때 다소 복잡하게 느껴지기도 한다. 하지만 쉬운 예제부터 하나씩 하다보면 언젠가 숙달될태니 그 시작을 KNN으로 해보자. 

`method = "knn"`는 model을 KNN 알고리즘으로 설정한다는 의미이며 `tuneLength`는 최적값을 판단할 k의 개수를 의미한다. 15개로 설정했으니 15개의 k 조합에 대한 결과가 나올 것이다. `metric = "Accuracy"`는 성능 지표를 "Accuracy"로 정의한다는 의미이며 이 지표로 optimal value를 결정한다. 

```{r, collapse=TRUE, fig.cap="k에 대한 accuracy 추세"}
# Reproducibility를 위한 set.seed
set.seed(1234)
knn_model <- train(diagnosis ~ ., data = train_st, method = "knn",
                   trControl = traincontrol,
                   tuneLength = 15, 
                   metric = "Accuracy")
# 결과
knn_model

# 최적의 k값
knn_model$bestTune

# k값에 따른 accuracy의 추세
plot(knn_model)
```
`train_st`와 `train()`을 이용해 KNN 알고리즘에서 가장 큰 정확도를 갖는 최적 k 값, `7`을 구했다. 이제 이 결과를 이용해 testing을 해보자. 


## 3.4 Evaluate the model with test set
위에서 구한 결과로 test set을 이용한 model 평가는 model을 test set에 fitting한 후 그 정확도를 판단하는 것으로 한다. 정확도는 confusion matrix로 쉽게 알아볼 수 있으며 이번 예제에서는 training보다 간단한 과정으로 모델을 평가할 수 있다. 

```{r, collapse=TRUE}
pred_test <- predict(knn_model, newdata = test_st)

confusionMatrix(pred_test, as.factor(test_st$diagnosis))
mean(pred_test == test_st$diagnosis) # Accuracy와 같음
```

평가 결과, training에서보다 더 높은 수준의 Accuracy를 갖는 것으로 보여 해당 예제에서 KNN 알고리즘은 상당히 좋은 성능을 갖다고 할 수 있다. 결과에서 보이는 몇 가지 metric에 대해 좀 더 알아보자. 

- No Information Rate : class에서 다수를 차지하는 factor의 비율을 의미한다. 이 경우, `diagnosis`에서 다수를 차지하는 `B`의 비율이다. 

- [P-Value [Acc > NIR]](https://cran.r-project.org/web/packages/caret/caret.pdf) : Acc > NIR에 대한 P-value로 class의 비율에 따른 Acc의 유효성 평가를 해볼 수 있다. 이 값은 단측 binomial test로 구한 P-value와 같다. 귀무가설은 `Acc <= NIR`이며 위의 예제는 `P-value < 0.05`의 상황이므로 Accuracy가 NIR보다 더 높다고 주장할 수 있다.

- Kappa : [Cohen's kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa)로 [confusion matrix에서 prediction과 reference가 같은 결론을 내릴 확률을 의미](https://thebook.io/006723/ch09/03/01/)한다. 높을수록 좋다. 

- [Mcnemar's Test P-Value](https://en.wikipedia.org/wiki/McNemar%27s_test) : [Mcnemar's Test](https://stats.stackexchange.com/questions/76875/what-is-the-difference-between-mcnemars-test-and-the-chi-squared-test-and-how/89415#89415)의 P-value로 `P-value >= 0.05` 이므로 예제의 경우, 비대각 요소의 확률이 같다고 말할 수 있다. 
    - [추가 링크](http://rasbt.github.io/mlxtend/user_guide/evaluate/mcnemar/)

- Sensitivity : Reference의 `B` 중 Prediction `B`와 일치하는 비율, 즉 Reference의 `B` class를 옳게 예측한 비율
- Specificity : Reference의 `M` 중 Prediction `M`와 일치하는 비율, 즉 Reference의 `M` class를 옳게 예측한 비율
- Positive Predictive Value(PPV) : Prediction의 `B` 중 Reference `B`와 일치하는 비율
- Negative Predictive Value(NPV) : Prediction의 `M` 중 Reference `M`와 일치하는 비율
    - 민감도와 특이도 그리고 PPV와 NPV를 한 세트처럼 이해하면 좋다. 
    - 민감도와 특이도는 Reference 기준으로 각 class의 예측 정확도를 평가하는 지표다. 
    - PPV와 NPV는 Prediction 기준으로 각 class를 얼마나 잘 예측했는지 평가하는 지표다. 
    - 추가 설명은 [링크 1](http://www.incodom.kr/Positive_Predictive_Value)과 [링크 2](https://3months.tistory.com/318)를 참고하길 바란다. 

- Prevalence : 전체에 대해서 Reference의 `B`와 일치하는 비율
- Detection Rate : 전체에 대해서 Reference과 Prediction의 `B`가 일치하는 비율
- Detection Prevalence : 전체에 대해서 Prediction의 `B`와 일치하는 비율
- Balanced Accuracy : Sensitivity와 Specificity의 평균

[수식](https://topepo.github.io/caret/measuring-performance.html#measures-for-predicted-classes)으로 이해하는 것이 빠를 수 있다. 구글링을 통해 검색해보면 의학 쪽의 예제가 많이 등장하는데 그런 예제로 이해하는 것 또한 하나의 방법이다. 


```{r, include=FALSE}
#  Mcnemar's Test P-Value 구해보기 
mat = as.table(rbind(c(88, 3), 
                     c(1, 50)))

result <- mcnemar.test(mat, correct=TRUE)

result$p.value # 0.6170751
# same as
1 - pchisq(0.25, 1)
```

## Summary 

지금까지 분류 문제를 해결하기 위한 knn 알고리즘 사용 방법을 알아보았다. 책과는 좀 다르게 caret 패키지를 사용해서 효율성을 높히는데 집중했으며 결과에서 볼 수 있는 값들에 대한 설명 또한 간단하게 했다. 

예제의 결과로 보았듯이 knn 알고리즘은 비교적 간단하지만 나쁘지 않은 성능을 보인다. 따라서 유클리디안 거리가 아닌 다른 거리 metric을 이용한 방법이나 가중치를 이용한 knn 알고리즘 또한 알아두길 추천하며 기회가 되면 다른 포스트에서 다루도록 하겠다. 


# 4장 확률적 학습 : 나이브 베이즈 분류(Naive Bayes Classification)

**사전확률**(prior)과 데이터로 얻은 확률의 조건부 확률을 **사후확률**(posterior)로 정의하는 **베이지안 확률론**(Bayesian probability)를 기반으로 분류문제를 해결하는 방법인 **나이브 베이즈 분류**(Naive Bayes Classification)는 비교적 간편한 사용법에 비해 뛰어난 효율로 여전히 많이 사용되고 있는 대표적인 **지도 학습**(Supervised learning) 방법입니다. 

책에서는 적당한 수준에서 베이즈 확률을 간단하게 설명하며 베이지안 기법에 대해 서술하고 있으며 **나이브 베이즈 알고리즘을 이용한 휴대폰 스팸 필터링**의 예제를 활용해 나이브 베이즈 분류 모형을 실습하도록 도와줍니다. 

책의 내용을 요약, 때로는 보충하며 나이브 베이즈 분류에 대해 정리해보도록 하겠습니다. 

## 4.1 베이지안 확률론에 대한 기초적인 설명

### 4.1.1 베이즈 정리(Bayes' theorem)
베이지안 확률론의 시작은 베이즈 정리로부터 시작한다고해도 크게 틀린 말은 아닐 것입니다. 실제로 베이즈 정리는 베이지안 확률의 핵심 개념인 **사전확률**과 **사후확률**을 사용하기 때문에 반드시 알아두어야 할 정리입니다. 

이해를 위해 [간단한 상황](https://www.mathsisfun.com/data/bayes-theorem.html)을 가정해보겠습니다. 

아침에 소풍을 가려고 준비했는데 구름이 잔뜩 끼어있습니다. 

- 모든 비 온 날의 50%는 아침에 구름부터 보였습니다.

- 그러나 구름이 끼는 날은 매우 평범한 날입니다. (40%의 아침에서 구름을 볼 수 있습니다.)

- 그리고 이번 달은 꽤 건조한 달입니다. (30일 중 3일, 그러니까 10%의 강우확률을 갖고 있습니다.)

**그렇다면 오늘, 피크닉에서 비를 맞을 확률은 얼마나 될까요?**


**확률**에 대해 공부할 때 각 사건이 일어날 확률을 명확히 정리하는 것은 매우 중요한 일입니다. 이 말이 저 말 같고 저 말이 이 말 같아서 헷갈리기 십상이기 때문입니다.  

- P(Rain) = 비가 올 확률

- P(Cloud) = 구름이 낄 확률

- **P(Rain ∩ Cloud) = 구름이 끼고 비가 올 확률**

- **P(Rain|Cloud) = 아침에 구름이 낀 후, 비가 올 확률**

- **P(Cloud|Rain) = 아침에 비가 온 후, 구름이 낄 확률**

```{r}
# https://stackoverflow.com/questions/35457611/optimized-venndiagram-with-internal-labels-r
x <- list()
x$`P(Rain)` <- as.character(c("P(Rain) - P(Rain ∩ Cloud)", "P(Rain ∩ Cloud)"))
x$`P(Cloud)` <- as.character(c("P(Cloud) - P(Rain ∩ Cloud)", "P(Rain ∩ Cloud)"))


v0 <- venn.diagram(x, fontfamily = "notosans", cex = 1.4, 
                   cat.fontfamily = "notosans", cat.cex = 1.8, cat.pos = 0,
                   fill = c("#6666FF", "#A0A0A0"), alpha = 0.5, filename = NULL)
overlaps <- calculate.overlap(x)
# overlaps <- rev(overlaps)

for (i in 1:length(overlaps)){
  v0[[i + 4]]$label <- paste(overlaps[[i]][[1]])
}

grid.newpage()
grid.draw(v0)
```

$$\begin{eqnarray} 
P(Rain|Cloud) &=& \frac{P(Rain \cap Cloud)}{P(Cloud)} 
\\ &=& \frac{P(Cloud|Rain) P(Rain)}{P(Cloud)} 
\\ &=& \frac{likelihood \times prior}{evidence} = posterior
\end{eqnarray}$$
